{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "# import nltk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess the data\n",
    "We strongly suggest that you can add pos tags use nltk.pos_tag() function. You can find more information about pos tags in the following link: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "def read_data(filename):\n",
    "    rows = []\n",
    "    with open(f'./ner/GMB/{filename}') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) < 2:\n",
    "                continue\n",
    "            rows.append(line.rstrip('\\n').split())\n",
    "    data = pd.DataFrame(rows, columns=['term', 'entitytags'])\n",
    "    # add the pos tags to the dataframe\n",
    "    # some lines of codes\n",
    "    # print(data.head())\n",
    "    tokens=[]\n",
    "    for word in data['term']:\n",
    "        tokens.append(word)\n",
    "    tokens2=nltk.pos_tag(tokens)\n",
    "    # print(data.tail())\n",
    "    # print(tokens2)\n",
    "    tokens3=[]\n",
    "    for x in tokens2:\n",
    "            tokens3.append(x[1])\n",
    "    data['pos'] = tokens3\n",
    "    # print(data.tail())\n",
    "    # print(tokens2)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('train')\n",
    "test = read_data('test')\n",
    "dev = read_data('dev')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process to get the train, test, dev dataset for crf\n",
    "\n",
    "def process_data(data):\n",
    "    dataset = []\n",
    "    sent = []\n",
    "    for i, (term, entitytags,pos) in data.iterrows():\n",
    "        if term == '.':\n",
    "            sent.append((term, entitytags,pos))\n",
    "            dataset.append(sent)\n",
    "            sent = []\n",
    "        else:\n",
    "            sent.append((term, entitytags,pos))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = process_data(train)\n",
    "test_sents = process_data(test)\n",
    "dev_sents = process_data(dev)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following function will design the feature for crf model. \n",
    "You need to add additional features to this function. The potential features you can add are:\n",
    "1. The characters of the word\n",
    "2. The pos tag of the word\n",
    "3. the word before and after the current word\n",
    "\n",
    "There will also be other features that you can add to improve the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file = 'src/glove.6B/glove.6B.300d.txt'  # path to the GloVe model file\n",
    "word2vec_output_file = 'src/glove.6B/glove.6B.300d.word2vec'  # path to the output word2vec model file\n",
    "\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "# Load vectors directly from the file\n",
    "model1 = gensim.models.KeyedVectors.load_word2vec_format('src/glove.6B/glove.6B.300d.word2vec',binary=False) ### Loading pre-trainned word2vec model\n",
    "### Embedding function \n",
    "def get_features(word):\n",
    "    word=word.lower()\n",
    "    try:\n",
    "         vector=model1[word]\n",
    "    except:\n",
    "        # if the word is not in vocabulary,\n",
    "        # returns zeros array\n",
    "        vector=np.zeros(300,)\n",
    "\n",
    "    return vector\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    emd = get_features(word)\n",
    "    \"\"\"\n",
    "    \n",
    "    Here we have already provided you some of the features. Without any modification, you can run the code and get the baseline result.\n",
    "    However, the performance is not good. We suggest you to add more features to improve the performance.\n",
    "    For example, you can add the character level features, the word shape features, the word embedding features, etc.\n",
    "    We also suggest you to add the features of the previous word and the next word.\n",
    "    \n",
    "    We strongly suggests you to add pos tags as features. You can use the pos tags provided by nltk.pos_tag() to get the pos tags of the words.\n",
    "    \n",
    "    \"\"\"\n",
    "    prev_word = sent[i-1][0]\n",
    "    vowels = [word.count(x) for x in \"aeiouAEIOU\"]\n",
    "    # next_word = sent[i+1][0]\n",
    "\n",
    "    # Load pre-trained word embeddingg\n",
    "\n",
    "    features = {\n",
    "        'word.lower()': word.lower(),\n",
    "        # add more features here\n",
    "        'pos_tags': sent[i][2],\n",
    "        'word_len': len(word),\n",
    "        'capital_start': word[0].isupper(),\n",
    "        'count_vowels': sum(vowels),\n",
    "        'word.isdigit': word.isdigit(),\n",
    "        'word.istitle': word.istitle(),\n",
    "        # 'current_word_title':word[0].title(),\n",
    "        'prev_word_len':len(prev_word),\n",
    "        'prev_word.lower':prev_word.lower(),\n",
    "        # 'word_emb': emb,\n",
    "        # 'prev_word_emb': prev_embedding,\n",
    "        'prev_word_tag':sent[i-1][2],\n",
    "        'prev_word_capital':prev_word[0].isupper(),\n",
    "        'prev_word.istitle':prev_word.istitle(),\n",
    "        'prev_word.isdigit': prev_word.isdigit()\n",
    "        # 'next_word.isdigit':next_word.isdigit(),\n",
    "        # 'next_word.istitle':next_word.istitle()\n",
    "    }\n",
    "    for iv,value in enumerate(emd):\n",
    "        features['word_emd{}'.format(iv)]=value\n",
    "    \n",
    "    if i < len(sent)-1:\n",
    "        next_word = sent[i+1][0]\n",
    "        features.update({\n",
    "            'next_word.isdigit':next_word.isdigit(),\n",
    "            'next_word.istitle':next_word.istitle()\n",
    "        })\n",
    "\n",
    "    return features\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label,pos in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label,pos in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent2features(train_sents[0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "X_dev = [sent2features(s) for s in dev_sents]\n",
    "y_dev = [sent2labels(s) for s in dev_sents]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code will help you visualize the feature for a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word.lower()': 'masked'},\n",
       " {'word.lower()': 'assailants'},\n",
       " {'word.lower()': 'with'},\n",
       " {'word.lower()': 'grenades'},\n",
       " {'word.lower()': 'and'},\n",
       " {'word.lower()': 'automatic'},\n",
       " {'word.lower()': 'weapons'},\n",
       " {'word.lower()': 'attacked'},\n",
       " {'word.lower()': 'a'},\n",
       " {'word.lower()': 'wedding'},\n",
       " {'word.lower()': 'party'},\n",
       " {'word.lower()': 'in'},\n",
       " {'word.lower()': 'southeastern'},\n",
       " {'word.lower()': 'turkey'},\n",
       " {'word.lower()': ','},\n",
       " {'word.lower()': 'killing'},\n",
       " {'word.lower()': '45'},\n",
       " {'word.lower()': 'people'},\n",
       " {'word.lower()': 'and'},\n",
       " {'word.lower()': 'wounding'},\n",
       " {'word.lower()': 'at'},\n",
       " {'word.lower()': 'least'},\n",
       " {'word.lower()': 'six'},\n",
       " {'word.lower()': 'others'},\n",
       " {'word.lower()': '.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block of code, we use try and except because the version of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code will help you visualize the learned features for crf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "# labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7766174794313391"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [sent2tokens(s) for s in test_sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [sent2labels(s) for s in test_sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for (word, true_id, pred_id) in zip(words, labels, y_pred):\n",
    "    for (w, t, p) in zip(word, true_id, pred_id):\n",
    "        line = ' '.join([w, t, p])\n",
    "        predictions.append(line)\n",
    "    predictions.append('')\n",
    "with open('crf_pred', 'w') as f:\n",
    "    f.write('\\n'.join(predictions))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "eval_script = '../released/src/conlleval'\n",
    "predf = 'crf_pred'\n",
    "scoref = 'crf_score'\n",
    "os.system('%s < %s > %s' % (eval_script, predf, scoref))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 12005 tokens with 1163 phrases; found: 880 phrases; correct: 566.\n",
      "accuracy:  89.81%; precision:  64.32%; recall:  48.67%; FB1:  55.41\n",
      "          company: precision:  73.42%; recall:  53.95%; FB1:  62.20  158\n",
      "         facility: precision:  67.69%; recall:  64.71%; FB1:  66.17  65\n",
      "          geo-loc: precision:  74.43%; recall:  65.88%; FB1:  69.89  262\n",
      "            movie: precision: 100.00%; recall:  14.29%; FB1:  25.00  2\n",
      "      musicartist: precision:  54.17%; recall:  23.21%; FB1:  32.50  24\n",
      "            other: precision:  52.05%; recall:  38.78%; FB1:  44.44  146\n",
      "           person: precision:  55.88%; recall:  54.91%; FB1:  55.39  170\n",
      "          product: precision:  50.00%; recall:  13.98%; FB1:  21.85  26\n",
      "       sportsteam: precision:  48.00%; recall:  30.77%; FB1:  37.50  25\n",
      "           tvshow: precision:   0.00%; recall:   0.00%; FB1:   0.00  2\n"
     ]
    }
   ],
   "source": [
    "eval_lines = [l.rstrip() for l in open(scoref, 'r', encoding='utf8')]\n",
    "\n",
    "for i, line in enumerate(eval_lines):\n",
    "    print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check what classifier learned:\n",
    "\n",
    "You will need to analyze how the transition the model is learned in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "B-movie -> I-movie 5.701027\n",
      "B-musicartist -> I-musicartist 5.685554\n",
      "B-facility -> I-facility 5.433560\n",
      "B-tvshow -> I-tvshow 5.124462\n",
      "I-tvshow -> I-tvshow 5.087995\n",
      "B-sportsteam -> I-sportsteam 5.070978\n",
      "I-musicartist -> I-musicartist 4.959611\n",
      "I-movie -> I-movie 4.915102\n",
      "B-person -> I-person 4.469746\n",
      "I-product -> I-product 4.219256\n",
      "B-company -> I-company 4.214045\n",
      "B-other -> I-other 4.149609\n",
      "I-facility -> I-facility 4.102881\n",
      "B-product -> I-product 4.084997\n",
      "I-company -> I-company 4.007464\n",
      "I-other -> I-other 3.847204\n",
      "B-geo-loc -> I-geo-loc 3.835153\n",
      "I-sportsteam -> I-sportsteam 3.246626\n",
      "O      -> O       2.816641\n",
      "I-geo-loc -> I-geo-loc 2.522798\n",
      "\n",
      "Top unlikely transitions:\n",
      "I-person -> I-product -2.034085\n",
      "B-geo-loc -> I-company -2.034881\n",
      "I-other -> I-product -2.041728\n",
      "B-person -> I-other -2.109488\n",
      "I-geo-loc -> I-other -2.161472\n",
      "B-geo-loc -> I-facility -2.285191\n",
      "B-company -> I-other -2.335745\n",
      "I-person -> I-other -2.406683\n",
      "I-person -> B-person -2.424812\n",
      "B-geo-loc -> I-other -2.845225\n",
      "O      -> I-tvshow -2.898649\n",
      "O      -> I-movie -3.463675\n",
      "O      -> I-sportsteam -3.489209\n",
      "O      -> I-musicartist -3.509705\n",
      "O      -> I-geo-loc -3.533981\n",
      "O      -> I-facility -3.697960\n",
      "O      -> I-company -3.769174\n",
      "O      -> I-person -3.926349\n",
      "O      -> I-product -4.759204\n",
      "O      -> I-other -5.086523\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the state features:\n",
    "\n",
    "You will need to analyze how your features will help the model to learn the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "6.156966 B-company word.lower():#talktalk\n",
      "5.709716 B-company word.lower():http://web.com\n",
      "5.171185 B-geo-loc word.lower():#chicago\n",
      "4.654741 B-geo-loc word.lower():#la\n",
      "4.649690 O        word.lower():rt\n",
      "4.505379 B-company word.lower():zendesk\n",
      "4.458371 B-other  word.lower():#isis\n",
      "4.285988 B-facility word.lower():#revelryroom\n",
      "4.238361 B-tvshow word.lower():#bb11\n",
      "4.163233 B-product word.lower():#theincredibletruestory\n",
      "4.137834 B-sportsteam word.lower():#bufvsphi\n",
      "4.119632 B-musicartist word.lower():#dreamlabrobot\n",
      "4.119521 B-sportsteam prev_word.lower:vs\n",
      "4.075986 B-geo-loc word.lower():china\n",
      "4.058030 I-geo-loc word.lower():mary's.\n",
      "4.039027 B-other  word.lower():#lds\n",
      "4.015493 B-facility word.lower():#dcnavyyard\n",
      "3.992438 B-movie  word.lower():#fridaythe13th\n",
      "3.974290 B-company word.lower():#katv7\n",
      "3.969215 B-company word.lower():linode\n",
      "3.938312 B-other  word.lower():christmas\n",
      "3.935788 B-other  word.lower():#daca\n",
      "3.899440 B-facility prev_word.lower:at\n",
      "3.874722 B-company word.lower():sendgrid\n",
      "3.860344 B-geo-loc word.lower():#santamonica\n",
      "3.848412 B-other  word.lower():#bottlerock2015\n",
      "3.833138 I-company prev_word.lower:wendy\n",
      "3.818895 B-movie  word.lower():#starwars\n",
      "3.813108 I-other  word.lower():vavavoom0328\n",
      "3.805131 B-facility prev_word.lower:@\n",
      "\n",
      "Top negative:\n",
      "-1.678027 O        word.lower():night\n",
      "-1.682594 O        word.lower():#usarmy\n",
      "-1.725730 O        prev_word.lower:everything\n",
      "-1.760963 O        word.lower():day\n",
      "-1.783572 O        prev_word.lower:near\n",
      "-1.789975 O        word.lower():times\n",
      "-1.793451 O        word.lower():#chattanooga\n",
      "-1.805500 B-person next_word.isdigit\n",
      "-1.809743 O        word.lower():un\n",
      "-1.810262 O        prev_word.lower:fashion\n",
      "-1.839970 O        prev_word.lower:enter\n",
      "-1.947921 O        prev_word.lower:football\n",
      "-2.018229 O        prev_word.lower:download\n",
      "-2.026393 O        word.lower():twitter\n",
      "-2.041719 B-tvshow prev_word_capital\n",
      "-2.049207 O        prev_word.lower:yeah\n",
      "-2.092305 O        prev_word.lower:#nowplaying\n",
      "-2.154784 O        prev_word.lower:h\n",
      "-2.171648 O        prev_word.lower:13\n",
      "-2.252408 O        prev_word.lower:n\n",
      "-2.299263 O        word.lower():uk\n",
      "-2.306348 O        prev_word.lower:em\n",
      "-2.313514 O        word.lower():rose\n",
      "-2.348490 B-person prev_word.lower:in\n",
      "-2.363683 O        prev_word.lower:national\n",
      "-2.501772 O        word.lower():ca\n",
      "-2.502040 O        prev_word.lower:bartleson\n",
      "-2.517840 O        prev_word.lower:north\n",
      "-2.538234 O        word.lower():page\n",
      "-2.661074 O        word.lower():10\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bec4ffc65abad1b1b7862a0abc19e670057a4673581208a7c61a208b38b7ad18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
